---
title: "Analysis plan"
author: "Lucija Batinović"
format: docx
editor: source
execute: 
  echo: FALSE
  message: FALSE
  warning: FALSE
  error: FALSE
---


## Model specifications

As the reporting conventions of single-case designs allow for raw data extraction, we will take advantage of this opportunity and conduct an individual-participant multilevel meta-analysis. Having access to raw data allows for greater flexibility in modeling variations between and within participants, which is of even greater importance in single-case designs, considering the low sample size and often very few data points. For that reason, we will model a Bayesian meta-analysis. 
There are multiple crucial assumptions to make when modeling time-series (i.e., repeated measures) designs: distribution of the outcome measure, time trends, autoregression, etc. There is little consensus on how to best approach dealing with outcome measures in single-case designs. Most commonly used effect size measures in SCD research belong to non-parametric effect size measures, e.g., PND (percentage of non-overlap data), LRR (logistic response ratio) and more recently developed parametric alternatives, e.g., WS-SMD and BS-SMD (standardized mean differences; developed to be comparable to group-design studies). These parametric effect size measures, although robust when assumptions cannot be certainly defined, generally assume non-existing time trends and autoregression, with a normal distribution of outcome measures (Pustejovsky, 2017). Shadish et al. (2011) found that autocorrelation size is inconclusive, albeit non-negligible, particularly in multiple-baseline designs. Furthermore, assuming normal distribution of the outcome measures is a risky feat in a research area that heavily utilizes counts and percentages as measures, which would be more appropriately explained under the Poisson or otherwise binomial families of distributions. We will further break down each parameter that will go into the model, justifying the final decisions for each one.


### Clusters 
Generally, we use beta(binomial) distributions to explain the data that is collected in trials and regarded as the number or rate of success(es) in a trial. Often, this can be seen in medical research on survival rates, or educational testing data. However, those trials are most often independent. In our case, the trials are sessions in a repeated-measure design, which makes the nested within participants. Without having to scale it further, this requires a hierarchical modelling to account for interdependence of these observations. A single case design then requires modelling between participant average to reach an effect size. That is our third level - study level, where the participants are nested within a study. Finally, as we want to meta-analyze findings from various studies, we calculate the population average effect based on the information from multiple studies. This leads us to the final, fourth level of this model that accounts for between study variation.
We are employing a random-intercept model on each level, with the first level being modeled as random due to the beta-binomial distribution accounting for over-dispersion by essentially allowing random probabilities for each trial.  

#### Outcome
Level one of the multilevel model describes the relationship between the outcome - word or non-word reading, and the predictor - reading intervention. Informed by previous reviews with similar interest, we assume majority of the studies will employ non-standardized word-reading tests, with counts and percentages as measures, and fixed word lists. If we considered the trials to be independent, binomial or Poisson distribution would potentially be the most appropriate model for the outcome. However, as we assume the existence of autocorrelation (but will not model it explicitly) and knowledge transfer, we cannot assume the trials are independent. To allow for greater flexibility and account for over-dispersion (i.e., variance being larger than the mean), and to account for unavailable raw counts in instances where the studies present proportion of correct responses, we will model the outcome under the beta-binomial distribution (Gelman et al., 2020).

This sets the starting equation as:
$$
Y_{i} \sim BetaBin(N,\mu,\theta)
$$
Beta-binomial distribution can be explained as a binomial distribution that has its probability parameter informed by the beta distribution of probabilities. Here $Y_{i}$  is the number or rate of correct responses for each individual i = 1, ..., n, $BetaBin$ indicates the observations are drawn from a beta-binomial distribution which has the $N$ parameter for the number of trials, $\mu$ for the average probability (Some authors denote it as pbar; McElreath, 2020) of the beta distribution and $\theta$ as the dispersion (precision) parameter of the beta distribution (some authors might note it as $\phi$; Burkner, 2017).
In case we had a dataset that contains observations of correct responses that is not nested within participants, classes, studies, schools, etc., this equation would be our model. One thing we would have to specify for the Bayesian model would be the priors for the distribution parameters.

$$
Y_{i} \sim BetaBin(N,\mu,\theta) 
$$
$$
logit(\mu) = \alpha_{treatment[i]}
$$
$$
\alpha \sim Normal(0, 1.5)
$$
$$
\theta \sim gamma(0.1,0.1)
$$
We take the first function and add a prior for the average probability parameter ($\mu$) which is a log odds probability of success ($\alpha_{treatment[i]}$), with the $treatment$ index implying whether it's baseline or intervention. We draw the $\alpha$ from a normal distribution with a $mean$ of 0 and $SD$ of 1.5. The choice of the distribution is relatively arbitrary, we can assume that the odds parameter can be drawn from a normal distribution, but parameters are up to us to define based on previous knowledge.

Now, we have the model that would explain how we get the observation. To calculate the effect for our meta-analysis, we need to account for clusters on the levels we specified in the beginning: participant and study. There are two ways to approach when we have raw data - calculate average effects per study and do a meta-analysis of those effects (which is the common way of conducting a meta-analysis when you don't have raw data) or build a model that directly calculates the population (average) effect from the raw data. We will do the latter, one stage approach.

We start with the same equation in the first line:

$$
Y_{i} \sim BetaBin(N,\mu,\theta)
$$
$$
logit(\mu) = \alpha_{treatment[i]} + g_{participant[i]} + g_{study[i]}
$$

Here, $\alpha_{treatment[i]}$ is modeled as:
$$
\alpha_{treatment[i]} = \alpha_0 + \alpha_1 × Treatment_[i]
$$
where $\alpha_0$ is the baseline log odds of success when no treatment is applied, and $\alpha_1$ represents the change in log odds due to the treatment. The term $Treatment_[i]$ is a variable indicating baseline or intervention stage.

$$
\alpha \sim Cauchy(\mu_{study},\sigma_{study})
$$

$$
\mu_{study} \sim Cauchy(\mu_{pop},\sigma_{pop})
$$
$$
\sigma_{study} \sim HalfStudent(\nu_{pop}, \mu_{pop},\sigma_{pop})
$$

$$
\mu_{pop} \sim Student(7, 0, 3)
$$

$$
\sigma_{pop} \sim HalfStudent(7, 0, 3)
$$

$$
\textbf{R} \sim LKJcorr(2)
$$

$$
\theta = \Phi
$$
$$
\Phi \sim gamma(1,1)
$$

The multilevel model now specifies random effects on the participant ($g_{participant[i]}$) and study level ($g_{study[i]}$). We also define parameters for priors for the means and standard deviations. As these effects are interdependent, the model also produces a correlation matrix to explain the covariance. 

LKJ, the Lewandowski-Kurowicka-Joe distribution is a probability distribution over positive-definite symmetric matrices with unit diagonals, i.e., correlation matrices (Gelman, 2020). We use this distribution as a prior for the correlation matrix of effects in the model, and set it as $LKJcorr(2)$ of 2, which makes extreme correlations less likely in the correlation matrix, but still allows for correlations between the effects. 

We draw standard deviations from the half-student-t distributions, instead of normal, as it allows the possibility of extreme (tail) values, which helps us accommodate to uncertainty of these values. By defining it as "half", we constrain the distribution only to positive values for the SD.

Given the way that the designs are bounding the possible answers to a small range, and often conduct interventions until 100% success rate is achieved, we can expect ceiling effects, large effects in the intervention phase and larger improvements (slope) for those with lower baseline results.   


### 
```{r}
library(tidyverse)
library(brms)
library(tidybayes)
library(bayesplot)
library(ggdist)
#library(rstanarm)
set.seed(2024)
# get code from: https://michael-franke.github.io/Bayesian-Regression/practice-sheets/11a-cheat-sheet.html
```

```{r}
#| label: data
# data will be cleaned here for analyses. We will filter out maintenance and generalization phases for the analysis. We will also filter data based on SCED design and run all models for each category of designs. 
data <- read_csv("mydata.csv") %>% 
  mutate(success_rate = round(success_rate*100, digits = 0),
         fail_rate = round(abs(fail_rate)*100, digits = 0)) %>% 
  filter(ID != "Maintenance")

#data %>% filter(ID == "intervention") %>% ggplot(aes(x = x, y = success_rate, color = case)) +
#  geom_point() +
#  geom_line() +
#  facet_wrap(~study, ncol = len())

```

```{r}
#| label: baes-model
#| output: FALSE
# "0 + variable" removes the intercept, i.e., shows a separate coefficient for the "ref" group (baseline) and the comparison, vs. having the average intercept and then the difference between groups - it also removes centering parametrization and I want to avoid that so keep in the intercept

# get_prior extracts all priors defined for the used model. model one will be run with default priors set by the package
prior <- get_prior(success_rate|trials(success_rate + fail_rate) ~ ID + (ID|study/case), 
                   data = data, family = beta_binomial())

# hierarchical model that has success rate as the outcome predicted for each phase (ID) when nested among participants (case) and study (case)
model_one <- brm(success_rate|trials(success_rate + fail_rate) ~ ID + (ID|study/case), 
                 data = data, 
                 family = beta_binomial(link = "logit"), # link function for 0 to 1 probabilities
                 prior = prior, # uses the default priors
                 warmup = 2000, iter = 4000,
                 control = list(adapt_delta = 0.99) # fix divergent transitions
          )


```

```{r}
# density plots per study, code taken from Solomon Kurz https://bookdown.org/content/3686/dichotomous-predicted-variable.html#robust-logistic-regression # 21.4.2.1 Example: Baseball again.
#nd <- 
#  data %>% 
#  group_by(study) %>% 
#  summarise(success = mean(success_rate) %>% round(0)) %>% 
#  # to help join with the draws
#  mutate(name = str_c("V", 1:n()))
#
## push the model through `fitted()` and wrangle
#fitted(model_one,
#       newdata = data,
#       re_formula = success_rate|trials(success_rate + fail_rate) ~ ID + (ID|study/case),
#       scale = "linear",
#       summary = F) %>% 
#  as_tibble() %>% 
#  pivot_longer(everything()) %>% 
#  mutate(probability = inv_logit_scaled(value)) %>% 
#  left_join(nd, by = "name") %>% 
#  
#  # plot
#  ggplot(aes(x = probability, y = study)) +
#  geom_vline(xintercept = fixef(model_one)[1] %>% inv_logit_scaled(), color = "orange") +
#  ggridges::geom_density_ridges(color = "red", fill = "pink", size = 1/2,
#                      rel_min_height = 0.005, scale = .9) +
#  geom_jitter(data = data,
#              aes(x = success_rate / (success_rate + fail_rate)),
#              height = .025, alpha = 1/6, size = 1/6, color = "red") +
#  scale_x_continuous("Correct Words / All attempts", breaks = 0:5 / 5, expand = c(0, 0)) +
#  coord_cartesian(xlim = c(0, 1), 
#                  ylim = c(1, 9.5)) +
#  ggtitle("Data with Posterior Predictive Distrib.") +
#  theme(axis.text.y = element_text(hjust = 0),
#        axis.ticks.y = element_blank())
```



```{r}
#| label: baes-plots
#get_variables(model_one) #extracts all posterior draw variables
summary(model_one)
color_scheme_set("red")
#posterior predictive check for baseline and intervention fit
pp_check(model_one, type = "dens_overlay_grouped", group = "ID", ndraws = 100)

#stancode(model_one, data = data, family = beta_binomial(link = "logit"),
#                 prior = prior) #shows stan code for the model
#general plots
plot(model_one)
# plot the posterior distributions per case nested within their studies
model_one %>%
  tidybayes::spread_draws(b_IDIntervention, `r_study:case`[`study:case`,IDIntervention]) %>% #extract posterior samples for the population effect and cases nested within study; presents them in a tidy format
  mutate(case_mean = exp(b_IDIntervention) + exp(`r_study:case`)) %>% #inverse logit to probabilities
  # mutate(case_mean = inv_logit_scaled(b_IDIntervention + `r_study:case`)) %>% #if we want to plot probabilities instead of OR
  ggplot(aes(y = `study:case`, x = case_mean)) +
  ggdist::stat_halfeye(color = "#dddbf1", fill = "#8c2f39") +
  geom_vline(xintercept = exp(fixef(model_one)[2]), color = "gray") +
  theme_minimal() +
  coord_cartesian(xlim = c(0, 80)) # set limits to be between 0 and 1 probability

# plot the posterior distributions of studies
model_one %>%
  tidybayes::spread_draws(b_IDIntervention, r_study[study,IDIntervention]) %>% #extract posterior samples for the population effect and cases nested within study; presents them in a tidy format
  mutate(case_mean = exp(b_IDIntervention) + exp(`r_study`)) %>% #inverse to probabilities
  # mutate(case_mean = inv_logit_scaled(b_IDIntervention + `r_study`)) %>% #if we want to plot probabilities instead of OR
  ggplot(aes(y = study, x = case_mean)) +
  ggdist::stat_halfeye(color = "#dddbf1", fill = "#8c2f39") +
  theme_minimal() + 
  coord_cartesian(xlim = c(0,80)) # set limits to be between 0 and 1 probability

```
```{r}
#| label: baes-model-one-forest
pop_effect <- as.data.frame(fixef(model_one)) %>% 
  rownames_to_column() %>% 
  pivot_wider(names_from = rowname, values_from = c(Estimate, Est.Error, Q2.5, Q97.5), names_sep = ".") %>% 
  mutate(rowname = "Population Effect")
random_effects_one <- ranef(model_one)
random_effects_one <- as.data.frame(random_effects_one[["study"]]) %>% 
  rownames_to_column()

forest_model_one <- bind_rows(pop_effect, random_effects_one) 

ggplot(forest_model_one, aes(x = Estimate.IDIntervention, y = rowname)) +
  geom_errorbar(aes(xmin = Estimate.IDIntervention - 2*Est.Error.IDIntervention, xmax = Estimate.IDIntervention + 2*Est.Error.IDIntervention, width = 0)) +
  geom_point(color = 'lavender') + 
  labs(x = 'Estimated Effect of Reading Intervention on Decoding in log-odds (+/- 2sd)', y = "Study") +
  geom_vline(xintercept = 0, color = '#8c2f39') +
  xlim(-8, 8)
  
```

# Main model
This model is built with more informed priors and will be used in the primary analysis
```{r}
#| label: baes-model-two
#| output: FALSE
prior_two <- get_prior(success_rate|trials(success_rate + fail_rate) ~ ID + (ID|study/case),
                   data = data, family = beta_binomial())

prior_two$prior[2] <- "student_t(7,0,3)" #fixed effects / slope; Brehm et al (2021)
prior_two$prior[3] <- "lkj(2)" # to keep higher correlations less likely
prior_two$prior[6] <- "cauchy(0,1)" # intercept
prior_two$prior[7] <- "gamma(1,1)" # phi prior
prior_two$prior[8] <- "student_t(7, 0, 3)" # more flexible and have better behavior near 0 (gelman, 2006), apparently we should avoid cauchy for sd as it leads to implausibly large values https://statmodeling.stat.columbia.edu/2018/04/03/justify-my-love/

model_two <- brm(success_rate|trials(success_rate + fail_rate) ~ ID + (ID|study/case), 
                 data = data, family = beta_binomial(link = "logit"),
                 prior = prior_two, warmup = 2000, iter = 4000,
                 control = list(adapt_delta = 0.99) # fix divergent transitions
          )

```


```{r}
#| label: baes-plots-two
summary(model_two)
color_scheme_set("red")
#posterior predictive check for baseline and intervention fit
pp_check(model_two, type = "dens_overlay_grouped", group = "ID", ndraws = 100)
#get_variables(model_one)
posterior_two <- as.matrix(model_two)

plot_title <- ggtitle("Posterior distributions",
                      "with medians and 90% intervals")
mcmc_areas(posterior_two,
           pars = c("b_Intercept", "b_IDIntervention", "sd_study:case__IDIntervention", "phi"),
           prob = 0.9) + plot_title
#general plots
plot(model_two)
#
model_two %>%
  tidybayes::spread_draws(b_IDIntervention, `r_study:case`[`study:case`,IDIntervention]) %>%
  mutate(case_mean = exp(b_IDIntervention) + exp(`r_study:case`)) %>% 
  #mutate(case_mean = inv_logit_scaled(b_IDIntervention + `r_study:case`)) %>% 
  ggplot(aes(y = `study:case`, x = case_mean)) +
  ggdist::stat_halfeye(color = "#8c2f39", fill = "#dddbf1") +
  theme_minimal() +
  coord_cartesian(xlim = c(-10,80))

```


```{r}
#| label: comparison-model
model_one_ic <- add_criterion(model_one, criterion = "loo")
model_one_waic <- waic(model_one)
model_two_ic <- add_criterion(model_two, criterion = "loo")
model_two_waic <- waic(model_two)

w <- loo_compare(model_one_waic, model_two_waic) 
print(w, simplify = F)

l <- loo_compare(model_one_ic, model_two_ic) # leave-one-out cross validation
print(l, simplify = F)

# moderator analyses
#model_gender_ic <- add_criterion(model_gender, criterion = "loo")
#model_gender_waic <- waic(model_gender)
#model_iq_ic <- add_criterion(model_iq, criterion = "loo")
#model_iq_waic <- waic(model_iq)
```


### Moderator analysis

#### IQ
```{r}
#| label: baes-model-iq
prior_iq <- get_prior(success_rate|trials(success_rate + fail_rate) ~ ID + (ID|study/case) + IQ, 
                   data = data, family = beta_binomial())

# the model is incorporating IQ as a moderator, set as fixed effect, and uses default priors
model_iq <- brm(success_rate|trials(success_rate + fail_rate) ~ ID + (ID|study/case) + IQ, 
                 data = data, family = beta_binomial(link = "logit"),
                 prior = prior_iq, warmup = 1000, iter = 4000
          )

summary(model_iq)

#posterior predictive check for baseline and intervention fit
pp_check(model_iq, type = "dens_overlay_grouped", group = "ID", ndraws = 100)

plot(model_iq)
```

```{r}
#| label: baes-plots-iq

#general plots
plot(model_iq)
get_variables(model_iq)
##
model_iq %>%
  tidybayes::spread_draws(b_IDIntervention, `r_study:case`[`study:case`,IDIntervention]) %>%
  mutate(case_mean = b_IDIntervention + `r_study:case`) %>% #not exponentiated
  ggplot(aes(y = `study:case`, x = case_mean)) +
  ggdist::stat_halfeye(color = "#8c2f39", fill = "#dddbf1") +
  theme_minimal() 

```

### Risk of Bias sensitivity analysis
```{r}
#| label: lo-rob
# do a sensitivity analysis based on risk of bias assessment
# data_rob <- data %>% filter(rob != "high")
```


### Exploratory analyses
# not in the manuscript
```{r}
#| label: EDA
# interaction of session (x) and phase (ID)
#prior_interaction <- get_prior(success_rate | trials(success_rate + fail_rate) ~ (1 | #study/case/ID/x), 
#                   data = data, family = beta_binomial())
#
## this models the interaction between phase and session
#model_interaction <- brm(success_rate | trials(success_rate + fail_rate) ~ (1 | #study/case/ID/x), #
#                 data = data, family = beta_binomial(link = "logit"),
#                 prior = prior_interaction, warmup = 1000, iter = 4000
#          )

# modelling all designs together with the design as moderator
#prior_design <- get_prior(success_rate|trials(success_rate + fail_rate) ~ ID + #(ID|study/case) + design, 
#                   data = data, family = beta_binomial())
#
## the model is incorporating age as a moderator, set as fixed effect, and uses default #priors
#model_age <- brm(success_rate|trials(success_rate + fail_rate) ~ ID + (ID|study/case) + #design, #
#                 data = data, family = beta_binomial(link = "logit"),
#                 prior = prior_design, warmup = 1000, iter = 4000
#          )
```

## References
Jané, M., Xiao, Q., Yeung, S., \*Ben-Shachar, M. S., \*Caldwell, A., \*Cousineau, D., \*Dunleavy, D. J., \*Elsherif, M., \*Johnson, B., \*Moreau, D., \*Riesthuis, P., \*Röseler, L., \*Steele, J., \*Vieira, F., *Zloteanu, M., & ^Feldman, G. (2024). Guide to Effect Sizes and Confidence Intervals. http://dx.doi.org/10.17605/OSF.IO/D8C4G
Pustejovsky, J. E., & Ferron, J. M. (2017). Research synthesis and meta-analysis of single-case designs. In J. M. Kaufmann, D. P. Hallahan, & P. C. Pullen (Eds.), Handbook of Special Education, 2nd Edition. New York, NY: Routledge.
Shadish, W. R., & Sullivan, K. J. (2011). Characteristics of single-case designs used to assess intervention effects in 2008. Behavior Research Methods, 43(4), 971–980. https://doi.org/10.3758/s13428-011-0111-y
Brehm, Z., Wagner, A., VonKaenel, E., Burton, D., Weisenthal, S. J., Cole, M., Pang, Y., & Thurston, S. W. (2021). Selection of inverse gamma and half-t priors for hierarchical models: Sensitivity and recommendations (arXiv:2108.12045). arXiv. http://arxiv.org/abs/2108.12045
Gelman, A. (2006). Prior distributions for variance parameters in hierarchical models (comment on article by Browne and Draper). Bayesian Analysis, 1(3), 515–534. https://doi.org/10.1214/06-BA117A

